# -*- coding: utf-8 -*-
"""Stock_Price_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DK50wV0SochMqmh-PdVawcAhtddNpKZ9
"""

#Step 1: Get Tesla prices (X)
#Step 1 B: Explore data (what will data will be taken into consideration for our model? what data is important?) (X)
# ---- volume and closing price 
#Step 1 C: Save the data into a csv to prevent running into exceeding API limits (X)
#Step 2: Determine how many days i want to predict the closing price (X)
#Step 2 B: Clean/ Pre-process  data (X)
#Step 2 C: Explore data (X)
#Step 3: Determine model that will be used (X)
#Step 3 B: Determine metrics to judge model performance (X)
#Step 4: Split data into testing and training (X)
#Step 5: Train the model (X)
#Step 7: Test Model (X)
#Step 8: Deploy model, Save parameters ()

"""# Import """

pip install alpha_vantage

"""## Introduction to Data"""

from sklearn.preprocessing import MinMaxScaler
from alpha_vantage.timeseries import TimeSeries
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import datetime
import math

key = input("Enter Alpha Vantage Key: ")
ts = TimeSeries(key, output_format='pandas')
df,meta = ts.get_daily('TSLA', outputsize='full')
df

df.info()

df.columns

df.describe()

"""**Notes**

- Data goes back to 2010
- No null values 
- Date is an index, not a column
- Might have to change data data type

## Cleaning
"""

clean_df = df.reindex(index=df.index[::-1])
clean_df = clean_df.rename({'1. open': 'open', '2. high':'high', '3. low':'low','4. close':'close' ,'5. volume':'volume'}, axis = 1)
clean_df['volume'] = clean_df['volume'].astype(float).astype(int)
clean_df.head(6)

clean_df.info()

"""## EDA"""

sns.displot(clean_df['close'], binwidth = 100)

"""**Skewed to the right**"""

sns.displot(clean_df['volume'], binwidth = 1000500)

"""**Skewed to the right**"""

plt.figure(figsize = (16,8))
plt.plot(clean_df['close'])
plt.title(' Tesla Stock Price over Time')
plt.xlabel('Date')
plt.ylabel('Price')
plt.show()

def get_moving_average(data,years) -> pd.DataFrame():
  moving_avg = []
  start = '{}-01-01'
  end = '{}-12-31'
  for year in years:
    yearly_data = data.loc[start.format(str(year)):end.format(str(year))]
    moving_avg.append(yearly_data['close'].mean())

  return pd.DataFrame(moving_avg, index = years, columns = ['Moving AVG'])

years = [2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022]
yearly_moving_avg = get_moving_average(clean_df,years)
yearly_moving_avg

plt.figure(figsize = (16,8))
plt.plot(years,yearly_moving_avg['Moving AVG'])
plt.title(' Tesla Stock Moving Average over Time')
plt.xlabel('Year')
plt.ylabel('Price')
plt.show()

"""**Observations**

- Gradual increase from 2010 -2014 
- Stagnation from 2014-2019 with slight increase
- Huge spike from 2019-2020
"""

#Do correlation matrix btw all varibles 
def get_average_volume(data,years) -> pd.DataFrame():
  volume_avg = []
  start = '{}-01-01'
  end = '{}-12-31'
  for year in years:
    yearly_data = data.loc[start.format(str(year)):end.format(str(year))]
    volume_avg.append(yearly_data['volume'].mean())

  return pd.DataFrame(volume_avg, index = years, columns = ['Volume AVG'])

yearly_vol_avg = get_average_volume(clean_df,years)
yearly_vol_avg

plt.figure(figsize = (16,8))
plt.plot(years,yearly_vol_avg['Volume AVG'])
plt.title(' Tesla Stock Volume Average over Time')
plt.xlabel('Year')
plt.ylabel('Volume')
plt.show()

"""**Observations**

- Shape increase from 2012-2013
- Decreasing 2013- 2015
- Gradual inc. 2015-2019
- Very dramatic increase from 2019-2020
"""

plt.figure(figsize=(16,6))
corr_heatmap = sns.heatmap(clean_df.corr(), annot = True)
corr_heatmap.set_title('Attribute Correlation')

"""**Volume closely correlated with closing price**

## Pre-Processsing
"""

#get closing price 
raw_data = clean_df[['close']]

#train and test set lengths 
train_len = math.ceil(len(raw_data) * .8)

# change dataset into array 
data = raw_data.values
data[:6]

#number of days needed to predict one day 
days_needed = 100

#scale data, value will be between 0 and 1
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(data)

#train test split 
train_set = scaled_data[:train_len]
test_set = scaled_data[train_len-days_needed:]


x_train = []
y_train = []

x_test = []
y_test = data[train_len:]

# appends [] with 100 days each in x_train
# appends the expected day after the 100 days are taken into consideration 

for i in range(days_needed, train_len):
  x_train.append(train_set[i-days_needed: i])
  y_train.append(train_set[i])

# appends [] with 100 days each 
for i in range(days_needed, len(test_set)):
  x_test.append(test_set[i-days_needed:i])

x_train[0][:6]

"""**15.80 went to 0 because it is the minimum value**"""

type(x_train)

# make sets numpy arrays 
x_train = np.array(x_train)
y_train = np.array(y_train)

x_test = np.array(x_test)

# reshape; need to be 3 dimensional for model input 
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
y_train = np.reshape(y_train, (y_train.shape[0], y_train.shape[1], 1))

x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))


print("x_train shape: ", x_train.shape)
print("y_train shape: ", y_train.shape)

print('x_test shape:', x_test.shape)

#build model 
#input_shape == (num days, 1)
model = Sequential()
model.add(LSTM(units = 50, return_sequences= True, input_shape = (x_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units = 100, return_sequences= False))
model.add(Dense(25))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')

model.fit(x = x_train, y = y_train, epochs = 30, batch_size=64)

"""# Testing"""

predictions = model.predict(x_test)
predictions = scaler.inverse_transform(predictions)

mse = ((predictions-y_test)**2).mean()
rmse = math.sqrt(mse)

print("MSE: ", mse)
print('RMSE: ', rmse)

"""**RMSE is higher than I would like it to be**"""

#all data sets 
train_df = clean_df
valid_df = clean_df[train_len:train_len+ days_needed]
test_df = clean_df[train_len+ days_needed:]

non_training_df = clean_df[train_len:]
non_training_df['predictions'] = predictions

valid_df.head()

test_df.head()

plt.figure(figsize=(16,8))
plt.plot(train_df['close'])
plt.plot(valid_df['close'])
plt.plot(non_training_df['predictions'])
plt.xlabel('year')
plt.title('Actual vs Predicted Tesla Stock Prices')
plt.ylabel('Price (USD)')
plt.legend(['Training (Actual)', 'Train/Test Overlap (Actual)', 'Predicted'])
plt.show()

"""*Some of the training set data is used in the x_test set (last 100 days) to predict the first day of y_test. For this reason, I've highlighted the overlap in data. The data that has never been introduced to the NN begins after the orange line. Therefore the predicted line that is seen after the orange line has no overlap in data from the training sets.*

**Observations**
* Predicted values very similar to the actual values in the train/test overlap 
* The model didn't do that well when estimating all time highs. 
* Overall, the model's predicted values followed the pattern of the actual values very well

# Forecast
"""

#Goal: Forecast share price for nth days 
def forecasting(model, scaled_data, numDays):
  #all predicted days 
  forecast= []

  # data used to predict 
  forecast_data = []
  forecast_data.append(scaled_data[-100:])
  forecast_data = np.array(forecast_data)
  forecast_data = np.reshape(forecast_data, (forecast_data.shape[0], forecast_data.shape[1], 1))

  for i in range(numDays):
    forecast.append(model.predict(forecast_data[0:, i:, 0:])[0])
    forecast_data = np.append(forecast_data, forecast[i])
    forecast_data = np.array(forecast_data)
    forecast_data = np.reshape(forecast_data, (1, forecast_data.shape[0], 1))
  return forecast

#number of days to forecast 
numDays = 30
forecast = forecasting(model = model, scaled_data=scaled_data, numDays=numDays)

#change data back into orginal unit 
forecast = scaler.inverse_transform(forecast)

#gets dates from start to end date; for Dataframe 
##TODO: TAKE A COOUNT FOR WEEKENDS, CHANGE FOR LOOP TO A WHILE LOOP 
def get_dates(start_date, numDays) -> list():
  dates = []
  end_date = start_date + datetime.timedelta(days = numDays)
  for i in range(delta.days + 1):
    day = start_date + timedelta(days=i)
    dates.append(str(day.date))
  return dates 

#get the day after the most recent date on df 
start_date = clean_df.index[-1].strftime('%y-%m-%d')
start_date = datetime.datetime.strptime(start_date, '%y-%m-%d')
start_date = start_date + datetime.timedelta(days = 1)

forecast_df_index = get_dates(start_date, numDays)

forecast_df_index[:30]

#TODO: CREATE DATAFRAME TO PLOT 
forecast_df = pd.DataFrame(forecast, columns = ['close'], index = forecast_df_index)
forecast.head()

#TODO: PLOT ACTUAL AGAINST FORECASTED 

plt.figure(figsize=(14, 8))
plt.plot(clean_df['close'])
plt.plot()
plt.legend(['Actual', ' 30 day Projection'])

#TODO: SAVE FORECAST AS CSV; MOVE TO POWER BI