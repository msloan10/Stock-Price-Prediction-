# -*- coding: utf-8 -*-
"""Stock_Price_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DK50wV0SochMqmh-PdVawcAhtddNpKZ9
"""

#Step 1: Get Tesla prices (X)
#Step 1 B: Explore data (what will data will be taken into consideration for our model? what data is important?) (X)
# ---- volume and closing price 
#Step 1 C: Save the data into a csv to prevent running into exceeding API limits (X)
#Step 2: Determine how many days i want to predict the closing price (X)
#Step 2 B: Clean/ Pre-process  data (X)
#Step 2 C: Explore data (X)
#Step 3: Determine model that will be used (X)
#Step 3 B: Determine metrics to judge model performance (X)
#Step 4: Split data into testing and training (X)
#Step 5: Train the model (X)
#Step 6: hyperparameters ()
#Step 7: Test Model ()
#Step 8: Deploy model, Save parameters ()

"""# Import """

pip install alpha_vantage

"""## Introduction to Data"""

from sklearn.preprocessing import MinMaxScaler
from alpha_vantage.timeseries import TimeSeries
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import math

key = input("Enter Alpha Vantage Key: ")
ts = TimeSeries(key, output_format='pandas')
df,meta = ts.get_daily('TSLA', outputsize='full')
df

df.info()

df.columns

df.describe()

"""**Notes**

- Data goes back to 2010
- No null values 
- Date is an index, not a column
- Might have to change data data type

## Cleaning
"""

clean_df = df.reindex(index=df.index[::-1])
clean_df = clean_df.rename({'1. open': 'open', '2. high':'high', '3. low':'low','4. close':'close' ,'5. volume':'volume'}, axis = 1)
clean_df['volume'] = clean_df['volume'].astype(float).astype(int)
clean_df.head(6)

clean_df.info()

"""## EDA"""

sns.displot(clean_df['close'], binwidth = 100)

"""**Skewed to the right**"""

sns.displot(clean_df['volume'], binwidth = 1000500)

"""**Skewed to the right**"""

plt.plot(clean_df['close'])
plt.title(' Tesla Stock Price over Time')
plt.xlabel('Date')
plt.ylabel('Price')
plt.show()

def get_moving_average(data,years) -> pd.DataFrame():
  moving_avg = []
  start = '{}-01-01'
  end = '{}-12-31'
  for year in years:
    yearly_data = data.loc[start.format(str(year)):end.format(str(year))]
    moving_avg.append(yearly_data['close'].mean())

  return pd.DataFrame(moving_avg, index = years, columns = ['Moving AVG'])

years = [2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022]
yearly_moving_avg = get_moving_average(clean_df,years)
yearly_moving_avg

plt.plot(years,yearly_moving_avg['Moving AVG'])
plt.title(' Tesla Stock Moving Average over Time')
plt.xlabel('Year')
plt.ylabel('Price')
plt.show()

"""**Observations**

- Gradual increase from 2010 -2014 
- Stagnation from 2014-2019 with slight increase
- Huge spike from 2019-2020
"""

#Do correlation matrix btw all varibles 
def get_average_volume(data,years) -> pd.DataFrame():
  volume_avg = []
  start = '{}-01-01'
  end = '{}-12-31'
  for year in years:
    yearly_data = data.loc[start.format(str(year)):end.format(str(year))]
    volume_avg.append(yearly_data['volume'].mean())

  return pd.DataFrame(volume_avg, index = years, columns = ['Volume AVG'])

yearly_vol_avg = get_average_volume(clean_df,years)
yearly_vol_avg

plt.plot(years,yearly_vol_avg['Volume AVG'])
plt.title(' Tesla Stock Volume Average over Time')
plt.xlabel('Year')
plt.ylabel('Volume')
plt.show()

"""**Observations**

- Shape increase from 2012-2013
- Decreasing 2013- 2015
- Gradual inc. 2015-2019
- Very dramatic increase from 2019-2020
"""

plt.figure(figsize=(16,6))
corr_heatmap = sns.heatmap(clean_df.corr(), annot = True)
corr_heatmap.set_title('Attribute Correlation')

"""**Volume closely correlated with closing price**

## Pre-Processsing
"""

#get closing price 
raw_data = clean_df[['close']]

#train and test set lengths 
train_len = math.ceil(len(raw_data) * .8)

# change dataset into array 
data = raw_data.values
data[:6]

#number of days needed to predict one day 
days_needed = 100

#scale data, value will be between 0 and 1
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(data)

#train test split 
train_set = scaled_data[:train_len]
test_set = scaled_data[train_len-days_needed:]


x_train = []
y_train = []

x_test = []
y_test = data[train_len:]

# appends [] with 100 days each in x_train
# appends the expected day after the 100 days are taken into consideration 

for i in range(days_needed, train_len):
  x_train.append(train_set[i-days_needed: i])
  y_train.append(train_set[i])

# appends [] with 100 days each 
for i in range(days_needed, len(test_set)):
  x_test.append(test_set[i-days_needed:i])

x_train[0][:6]

"""**15.80 went to 0 because it is the minimum value**"""

type(x_train)

# make sets numpy arrays 
x_train = np.array(x_train)
y_train = np.array(y_train)

x_test = np.array(x_test)

# reshape; need to be 3 dimensional for model input 
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
y_train = np.reshape(y_train, (y_train.shape[0], y_train.shape[1], 1))

x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))


print("x_train shape: ", x_train.shape)
print("y_train shape: ", y_train.shape)

print('x_test shape:', x_test.shape)

#build model 
#input_shape == (num days, 1)
model = Sequential()
model.add(LSTM(units = 50, return_sequences= True, input_shape = (x_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units = 100, return_sequences= False))
model.add(Dense(25))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')

model.fit(x = x_train, y = y_train, epochs = 15, batch_size=64)

"""# Testing"""