# -*- coding: utf-8 -*-
"""Stock_Price_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DK50wV0SochMqmh-PdVawcAhtddNpKZ9
"""

#Step 1: Get Tesla prices (X)
#Step 1 B: Explore data (what will data will be taken into consideration for our model? what data is important?) (X)
# ---- volume and closing price 
#Step 1 C: Save the data into a csv to prevent running into exceeding API limits (X)
#Step 2: Determine how many days i want to predict the closing price (X)
#Step 2 B: Clean/ Pre-process  data (X)
#Step 2 C: Explore data (X)
#Step 3: Determine model that will be used ()
#Step 3 B: Determine metrics to judge model performance ()
#Step 4: Split data into testing and training (validation part of training set) ()
#Step 5: Train the model ()
#Step 6: Use validation set to determine hyperparameters ()
#Step 7: Test Model ()
#Step 8: Deploy model, Save parameters ()

"""# Import """

pip install alpha_vantage

"""## Introduction to Data"""

from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler
from alpha_vantage.timeseries import TimeSeries
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import math
import csv

key = input("Enter Alpha Vantage Key: ")
ts = TimeSeries(key, output_format='pandas')
data,meta = ts.get_daily('TSLA', outputsize='full')
data

data.info()

data.columns

data.describe()

"""**Notes**

- Data goes back to 2010
- No null values 
- Date is an index, not a column
- Might have to change data data type

## Cleaning
"""

clean_data = data.reindex(index=data.index[::-1])
clean_data = clean_data.rename({'1. open': 'open', '2. high':'high', '3. low':'low','4. close':'close' ,'5. volume':'volume'}, axis = 1)
clean_data['volume'] = clean_data['volume'].astype(float).astype(int)
clean_data.head()

clean_data.info()

"""## EDA"""

sns.displot(clean_data['close'], binwidth = 100)

"""**Skewed to the right**"""

sns.displot(clean_data['volume'], binwidth = 1000500)

"""**Skewed to the right**"""

plt.plot(clean_data['close'])
plt.title(' Tesla Stock Price over Time')
plt.xlabel('Date')
plt.ylabel('Price')
plt.show()

def get_moving_average(data,years) -> pd.DataFrame():
  moving_avg = []
  start = '{}-01-01'
  end = '{}-12-31'
  for year in years:
    yearly_data = data.loc[start.format(str(year)):end.format(str(year))]
    moving_avg.append(yearly_data['close'].mean())

  return pd.DataFrame(moving_avg, index = years, columns = ['Moving AVG'])

years = [2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022]
yearly_moving_avg = get_moving_average(clean_data,years)
yearly_moving_avg

plt.plot(years,yearly_moving_avg['Moving AVG'])
plt.title(' Tesla Stock Moving Average over Time')
plt.xlabel('Year')
plt.ylabel('Price')
plt.show()

"""**Observations**

- Gradual increase from 2010 -2014 
- Stagnation from 2014-2019 with slight increase
- Huge spike from 2019-2020
"""

#Do correlation matrix btw all varibles 
def get_average_volume(data,years) -> pd.DataFrame():
  volume_avg = []
  start = '{}-01-01'
  end = '{}-12-31'
  for year in years:
    yearly_data = data.loc[start.format(str(year)):end.format(str(year))]
    volume_avg.append(yearly_data['volume'].mean())

  return pd.DataFrame(volume_avg, index = years, columns = ['Volume AVG'])

yearly_vol_avg = get_average_volume(clean_data,years)
yearly_vol_avg

plt.plot(years,yearly_vol_avg['Volume AVG'])
plt.title(' Tesla Stock Volume Average over Time')
plt.xlabel('Year')
plt.ylabel('Volume')
plt.show()

"""**Observations**

- Shape increase from 2012-2013
- Decreasing 2013- 2015
- Gradual inc. 2015-2019
- Very dramatic increase from 2019-2020
"""

plt.figure(figsize=(16,6))
corr_heatmap = sns.heatmap(clean_data.corr(), annot = True)
corr_heatmap.set_title('Attribute Correlation')

"""**Volume closely correlated with closing price**

## Train, Test Split
"""

#get closing prce 
exp_data = clean_data[['close']]

#train and test set lengths 
train_len = math.ceil(len(exp_data) * .8)
test_len = len(exp_data) - train_len

# change dataset into array 
exp_data_arr = exp_data.values

#scale data, value will be between 0 and 1
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(exp_data_arr)

#train test split 
train_set = scaled_data[:train_len]
test_set = scaled_data[train_len::]

x_train = []
y_train = []

# get last 100 days 
for i in range(100, train_len):
  x_train.append(train_set[i-100: i])
  y_train.append(train_set[i])